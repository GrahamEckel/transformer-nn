{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from semanticscholar import SemanticScholar\n",
    "# import pandas as pd\n",
    "import glob\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "# import csv\n",
    "import os\n",
    "import shutil\n",
    "# import itertools\n",
    "import sys\n",
    "# import time\n",
    "#from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initial Parameters \n",
    "#### Global Vars & Flags\n",
    "\n",
    "# replace with query results length and parse length using modulo\n",
    "\n",
    "offset = 0\n",
    "limit = 1\n",
    "query = \"WNT+signalling\"\n",
    "increment = 1\n",
    "NumResults = 100\n",
    "s2_api_key = 'qZWKkOKyzP5g9fgjyMmBt1MN2NTC6aT61UklAiyw'\n",
    "\n",
    "#### path variables \n",
    "# data_main = True\n",
    "# if data_main:\n",
    "#     root_path = \"E:\\Data\\Kaggle-Covid\\document_parses\" # Graham's \n",
    "# else:\n",
    "#     root_path = \"C:\\My files\\Courses\\CIS6050\\Data\\document_parses\" # Abdullah's\n",
    "\n",
    "# paths_csv = True\n",
    "\n",
    "# # temp vars \n",
    "# source_dir1 = \"E:\\Data\\Kaggle-Covid\\document_parses\\pdf_json\"\n",
    "# source_dir2 = \"E:\\Data\\Kaggle-Covid\\document_parses\\pmc_json\"\n",
    "# dest_dir = \"E:\\Data\\Kaggle-Covid\\document_parses\\\\abstracts_json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UTILS\n",
    "\n",
    "# Create json file\n",
    "def write_to_json(file_in_any, file_out_json):\n",
    "    with open(file_out_json + \".json\", \"w\") as outfile:\n",
    "        json.dump(file_in_any, outfile)\n",
    "        \n",
    "# Copy files from one directory to another\n",
    "def copy_file_to_dir(source, destinination):\n",
    "    for filename in glob.glob(os.path.join(source_dir2, '*.*')):\n",
    "        shutil.copy(filename, dest_dir) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Fields:\n",
    "- offset: enumerate start\n",
    "- limit: enumerate end (max 100)\n",
    "- paperId - Always included\n",
    "- authorId - Always included\n",
    "- name - Always included\n",
    "- title - Included if no fields are specified\n",
    "- authors - Up to 500 will be returned\n",
    "- externalIds\n",
    "- url\n",
    "- abstract\n",
    "- venue\n",
    "- year\n",
    "- referenceCount\n",
    "- citationCount\n",
    "- influentialCitationCount\n",
    "- isOpenAccess\n",
    "- fieldsOfStudy\n",
    "- s2FieldsOfStudy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modulo the starting and ending query numbers to arrive at 100, 1000, etc increments\n",
    "def get_data(query=str, offset=int, limit=int, increment=int, NumResults=int):\n",
    "    '''\n",
    "    query: the string to be searched on Semantic Search\n",
    "    offset: The starting result to retrieve\n",
    "    limit: The final result to retrieve\n",
    "    '''\n",
    "    \n",
    "    while limit <= NumResults:\n",
    "        url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&fields=abstract,authors,fieldsOfStudy&offset={offset}&limit={limit}\"\n",
    "        with urllib.request.urlopen(url) as webaddress:\n",
    "            data = json.loads(webaddress.read().decode())\n",
    "            #print(data)\n",
    "        \n",
    "        offset += increment\n",
    "        limit += increment\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(query, offset, limit, increment, NumResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 10000\n",
    "limit = 100\n",
    "query = \"WNT+signalling\"\n",
    "increment = 100\n",
    "NumResults = 20000\n",
    "s2_api_key = 'qZWKkOKyzP5g9fgjyMmBt1MN2NTC6aT61UklAiyw'\n",
    "\n",
    "data = []\n",
    "\n",
    "while offset <= NumResults:\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&fields=abstract,authors,fieldsOfStudy&offset={offset}&limit={limit}\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url,timeout=3)\n",
    "        r.raise_for_status()\n",
    "        with urllib.request.urlopen(url) as webaddress:\n",
    "            response = json.loads(webaddress.read().decode())\n",
    "            \n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print (\"Http Error:\",errh)\n",
    "        print(offset)\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print (\"Error Connecting:\",errc)\n",
    "        print(offset)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print (\"Timeout Error:\",errt)\n",
    "        print(offset)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print (\"OOps: Something Else\",err)\n",
    "        print(offset)\n",
    "        \n",
    "    data.extend(response[\"data\"])\n",
    "    offset += increment   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data))\n",
    "print(len(data))\n",
    "#print(data)\n",
    "write_to_json(data, \"authorIDs_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the dataset from Requests for unique AuthorIDs\n",
    "## extracting authorIDs from paper.json, storing in a set\n",
    "\n",
    "#data = get_data(query, offset, limit, increment, NumResults)\n",
    "\n",
    "authorIDs = []\n",
    "with open(\"authorIDs_test.json\", 'r', encoding='utf-8') as papers:\n",
    "    papers_data = json.load(papers)\n",
    "    #print(type(papers_data) \n",
    "    for idx, paper in tqdm(enumerate(papers_data), total=len(papers_data)):        \n",
    "        try:\n",
    "            for author in paper[\"authors\"]:\n",
    "                try:\n",
    "                    if author[\"authorId\"]:\n",
    "                        authorIDs.append(author[\"authorId\"])\n",
    "                    else: \n",
    "                        continue\n",
    "                except:\n",
    "                    #print the error message from sys\n",
    "                    print(sys.exc_info()[0])  \n",
    "        except:   \n",
    "            #print the error message from sys\n",
    "            print(sys.exc_info()[0])\n",
    "            \n",
    "print(len(authorIDs))\n",
    "aID = set(authorIDs)\n",
    "authorIDs_final = list(aID)\n",
    "print(len(authorIDs_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(authorIDs_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 req/sec transfer rate.\n",
    "\n",
    "t = time.process_time()\n",
    "sch = SemanticScholar(api_key=s2_api_key)\n",
    "count = 0\n",
    "authors_abstracts = []\n",
    "for authorID in authorIDs_final[:1000]:\n",
    "    if not (count / 100).is_integer():\n",
    "        authors_abstracts.append(sch.author(authorID))\n",
    "        count += 1\n",
    "        print(\"test2\")\n",
    "    else:\n",
    "        authors_abstracts.append(sch.author(authorID))\n",
    "        count += 1\n",
    "        print(\"test2\")\n",
    "write_to_json(authors_abstracts, \"authors_abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 req/sec transfer rate.\n",
    "\n",
    "t = time.process_time()\n",
    "sch = SemanticScholar(api_key=s2_api_key)\n",
    "authors_abstracts = []\n",
    "i = 0\n",
    "for i in range(0, len(authorIDs_final[:2000])):\n",
    "    try:\n",
    "        if not (i / 100).is_integer(): \n",
    "            authors_abstracts.append(sch.author(authorIDs_final[i]))\n",
    "            #time.sleep(3.1)\n",
    "            i += 1\n",
    "        else: \n",
    "            authors_abstracts.append(sch.author(authorIDs_final[i]))\n",
    "            print(f'author info retrieved: {i}')\n",
    "            #time.sleep(3.1)\n",
    "            i += 1\n",
    "    \n",
    "    except:\n",
    "        # print the error message from sys\n",
    "        print(sys.exc_info()[0], \"continue\")\n",
    "        print(i)\n",
    "        i -= 1\n",
    "        #print(sys.exc_info()[0])\n",
    "\n",
    "\n",
    "write_to_json(authors_abstracts, \"authors_abstracts\")\n",
    "duration = time.process_time() - t\n",
    "print(f\"completed after {duration} seconds\")   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(authors_abstracts))\n",
    "#print(type(authors_abstracts))\n",
    "#print(authors_abstracts[70:74])\n",
    "#write_to_json(authors_abstracts[70:74], \"authors_abstracts_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = SemanticScholar(api_key=s2_api_key)\n",
    "count = 0\n",
    "authors_abstracts = []\n",
    "for authorID in authorIDs_final[:100]:\n",
    "    try:\n",
    "\n",
    "        if (t % 5) == 0:\n",
    "            authors_abstracts.append(sch.author(authorID))\n",
    "            #time.sleep(3.1)\n",
    "            \n",
    "            write_to_json(authors_abstracts, \"authors_abstracts\") \n",
    "            print(f\"wrote author info to json at {t} minute\")\n",
    "            \n",
    "            count+=1\n",
    "            print(f'author info retrieved: {count}')\n",
    "        \n",
    "        else:\n",
    "            authors_abstracts.append(sch.author(authorID))\n",
    "            #time.sleep(3.1)\n",
    "            count+=1\n",
    "            print(f'author info retrieved: {count}')\n",
    "            \n",
    "    except:\n",
    "        # print the error message from sys\n",
    "        print(sys.exc_info()[0], \"continue\")\n",
    "        print(sys.exc_info()[0]) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Wrap into a query function with input: data\n",
    "\n",
    "# use the authorID_list to crawl semantic scholar for all papers by a specific author\n",
    "\n",
    "sch = SemanticScholar(api_key=s2_api_key)\n",
    "def SS_query(sch, data=list):\n",
    "    '''\n",
    "    Hits the Semantic Scholar API and returns all abstracts per author \n",
    "    sch: The semantic scholar library object\n",
    "    data: The result from Requests after parsed for authorIDs\n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    authors_abstracts = []\n",
    "    for authorID in data[:100]:\n",
    "        try:\n",
    "            t = datetime.now().minute\n",
    "            if (t % 5) == 0:\n",
    "                authors_abstracts.append(sch.author(authorID))\n",
    "                #time.sleep(3.1)\n",
    "                \n",
    "                write_to_json(authors_abstracts, \"authors_abstracts\") \n",
    "                print(f\"wrote author info to json at {t} minute\")\n",
    "                \n",
    "                count+=1\n",
    "                print(f'author info retrieved: {count}')\n",
    "            \n",
    "            else:\n",
    "                authors_abstracts.append(sch.author(authorID))\n",
    "                #time.sleep(3.1)\n",
    "                count+=1\n",
    "                print(f'author info retrieved: {count}')\n",
    "                \n",
    "        except:\n",
    "            # print the error message from sys\n",
    "            print(sys.exc_info()[0], \"continue\")\n",
    "            print(sys.exc_info()[0]) \n",
    "            \n",
    "    write_to_json(authors_abstracts, \"authors_abstracts\")\n",
    "    print(f\"wrote author info to json at {t} minute\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### DO NOT RUN UNLESS SURE ####\n",
    "\n",
    "# ## loading cleaned papers data to pandas\n",
    "# papers_path = \"E:\\Data\\Kaggle-Covid\\clean_df.csv\"\n",
    "# papers_df = pd.read_csv(papers_path)\n",
    "# #papers_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### DO NOT RUN UNLESS SURE ####\n",
    "\n",
    "# ## hitting the semanticScholar API using the paper IDs from papers_df to retrieve authorID\n",
    "# ## store AuthorID in a json \n",
    "# sch = SemanticScholar(timeout=2)\n",
    "# paperIDs_list = papers_df['paper_id'].tolist()\n",
    "# authorID = []\n",
    "\n",
    "# count = 0\n",
    "# papers = []\n",
    "# for paperID in paperIDs_list[:12500]:\n",
    "#     try:\n",
    "#         t = datetime.now().minute\n",
    "#         if (t % 5) == 0:\n",
    "#             papers.append(sch.paper(paperID))\n",
    "#             time.sleep(3.1)\n",
    "            \n",
    "#             write_to_json(papers, \"papers\") \n",
    "#             print(f\"wrote papers to json at {t} minute\")\n",
    "            \n",
    "#             count+=1\n",
    "#             print(f'papers retrieved: {count}')\n",
    "        \n",
    "#         else:\n",
    "#             papers.append(sch.paper(paperID))\n",
    "#             time.sleep(3.1)\n",
    "#             count+=1\n",
    "#             print(f'papers retrieved: {count}')\n",
    "            \n",
    "#     except:\n",
    "#         # print the error message from sys\n",
    "#         print(sys.exc_info()[0])\n",
    "#         time.sleep(330)   \n",
    "#         continue\n",
    "    \n",
    "# write_to_json(papers, \"papers\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOT RUN UNLESS NECESSARY ####\n",
    "# creating the unique list of authors\n",
    "\n",
    "# #parse the data from get_data to retrive the AuthorIDs\n",
    "# #sort and dedupe the list\n",
    "# authorID_list = data['author_id'].tolist()\n",
    "\n",
    "# json_dir = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "# authors_list = []\n",
    "# for file in json_dir:\n",
    "#     with open(file, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "#         for item in data[\"metadata\"][\"authors\"]:\n",
    "#             if item not in authors_list:\n",
    "#                 authors_list.append(item)\n",
    "                \n",
    "# # print(len(authors_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## hitting the semanticScholar API using the author IDs from authorIDs_list to retrieve all author info + abstracts\n",
    "# ## store author info in a json \n",
    "\n",
    "# sch = SemanticScholar(timeout=2)\n",
    "# authorIDs_list = woduplicates = list(authorIDs_final)\n",
    "\n",
    "# count = 0\n",
    "# authors_abstracts = []\n",
    "# for authorID in authorIDs_list[:12500]:\n",
    "#     try:\n",
    "#         t = datetime.now().minute\n",
    "#         if (t % 5) == 0:\n",
    "#             authors_abstracts.append(sch.author(authorID))\n",
    "#             time.sleep(3.1)\n",
    "            \n",
    "#             write_to_json(authors_abstracts, \"authors_abstracts\") \n",
    "#             print(f\"wrote author info to json at {t} minute\")\n",
    "            \n",
    "#             count+=1\n",
    "#             print(f'author info retrieved: {count}')\n",
    "        \n",
    "#         else:\n",
    "#             authors_abstracts.append(sch.author(authorID))\n",
    "#             time.sleep(3.1)\n",
    "#             count+=1\n",
    "#             print(f'author info retrieved: {count}')\n",
    "            \n",
    "#     except:\n",
    "#         # print the error message from sys\n",
    "#         print(sys.exc_info()[0], \"continue\")\n",
    "#         print(sys.exc_info()[0]) \n",
    "         \n",
    "#         # if isinstance(sys.exc_info()[0], type):\n",
    "#         #     print(sys.exc_info()[0], \"continue\") \n",
    "#         #     continue\n",
    "#         # else:\n",
    "#         #     print(sys.exc_info()[0]) \n",
    "#         #     time.sleep(330)  \n",
    "    \n",
    "# write_to_json(authors_abstracts, \"authors_abstracts\")\n",
    "# print(f\"wrote author info to json at {t} minute\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### NOT RUN UNLESS NECESSARY #### \n",
    "# # WIP: authors are matched and text + id is added to json based on the match. Takes forever. Still WIP\n",
    "\n",
    "# id = 0\n",
    "# with open(\"Paths_JSON_clean.csv\", \"r\") as csv_file:\n",
    "#     csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    \n",
    "#     # itertools.islice let's us take only a couple items from csv_object item\n",
    "#     for row in itertools.islice(csv_reader, 20):\n",
    "        \n",
    "#         # there will be missing look ups\n",
    "#         try:\n",
    "            \n",
    "#             # flattens the iterator list item to string\n",
    "#             filename = (''.join(row))\n",
    "#             abstract_dir = str(root_path) + \"/abstracts_json/\" + filename  \n",
    "#             print(abstract_dir)\n",
    "            \n",
    "#             # open up the abstracts json using the filename \n",
    "#             with open(abstract_dir, 'r', encoding='utf-8') as abstract:\n",
    "#                 abstract_data = json.load(abstract)\n",
    "                \n",
    "#                 # checks if an abstract exists\n",
    "#                 for text in abstract_data[\"abstract\"]:\n",
    "#                     if text and abstract_data[\"metadata\"][\"authors\"]:\n",
    "#                         for author in abstract_data[\"metadata\"][\"authors\"]:\n",
    "#                             #print(len(author))\n",
    "#                             with open(\"authors_test.json\", 'r', encoding='utf-8') as authors:\n",
    "#                                 authors_data = json.load(authors)\n",
    "#                                 if author in authors_data:\n",
    "#                                     id += 1\n",
    "#                                     loc = authors_data.index(author)  \n",
    "#                                     print(text)\n",
    "#                                     print(id)   \n",
    "#                                     print(loc)\n",
    "                                    \n",
    "#                                     #print(text)\n",
    "#                                 #print(author)\n",
    "#                                 #print(type(authors_data))\n",
    "#                         print(type(text))\n",
    "#                     else:\n",
    "#                         continue       \n",
    "#         except:\n",
    "            \n",
    "#             # print the error message from sys\n",
    "#             print(sys.exc_info()[0])\n",
    "            \n",
    "#             continue    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01dcfc14ced2cd3f3ca8b1eab5863829d3299fc9b429a7e61faa3aaa90451b9a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('NLP-startup': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
