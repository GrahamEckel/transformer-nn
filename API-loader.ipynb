{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from semanticscholar import SemanticScholar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions/Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create json file\n",
    "def write_to_json(file_in_any, file_out_json):\n",
    "    with open(file_out_json + \".json\", \"w\") as outfile:\n",
    "        json.dump(file_in_any, outfile)\n",
    "        \n",
    "# Copy files from one directory to another\n",
    "def copy_file_to_dir(source, destination):\n",
    "    for filename in glob.glob(os.path.join(source, '*.*')):\n",
    "        shutil.copy(filename, destination) \n",
    "\n",
    "# Hit the Semantic Scholar API and retrieve up to 10,000 papers via search terms. \n",
    "def get_data(query=str, offset=int, limit=int, NumResults=int):\n",
    "    '''\n",
    "    There is a hard limit of 10,000 papers per query. The sum of 'offset' and 'limit' must be < 10,000. \n",
    "    query: the string to be searched on Semantic Scholar\n",
    "    offset: What result/paper to start retrieval from the query results\n",
    "    limit: The length of results to retrieve at one time. Also functions as retrieval increments. Max 100.\n",
    "    '''\n",
    "\n",
    "    data = []\n",
    "\n",
    "    while offset <= NumResults:\n",
    "        url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&fields=authors&offset={offset}&limit={limit}\"\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url,timeout=15)\n",
    "            r.raise_for_status()\n",
    "            with urllib.request.urlopen(url) as webaddress:\n",
    "                response = json.loads(webaddress.read().decode())\n",
    "            time.sleep(3.3) # to work around the rate limit\n",
    "            data.extend(response[\"data\"])\n",
    "                \n",
    "        except requests.exceptions.HTTPError as errh:\n",
    "            print (\"Http Error:\", errh)\n",
    "            code = errh.response.status_code\n",
    "            if code == 403:\n",
    "                print(\"Timed out at:\", offset)\n",
    "                time.sleep(330) # to work around 403 errors\n",
    "                offset -= limit    \n",
    "            else:\n",
    "                offset += limit  \n",
    "                continue       \n",
    "\n",
    "        except requests.exceptions.ConnectionError as errc:\n",
    "            print (\"Error Connecting:\", errc)\n",
    "            print(\"failed at:\", offset)\n",
    "\n",
    "        except requests.exceptions.Timeout as errt:\n",
    "            print (\"Timeout Error:\", errt)\n",
    "            print(\"failed at:\", offset)            \n",
    "\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            print (\"Oops: Something Else\", err)\n",
    "            print(\"failed at:\", offset)\n",
    "        \n",
    "        offset += limit     \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "limit = 100\n",
    "NumResults = 9900\n",
    "s2_api_key = 'qZWKkOKyzP5g9fgjyMmBt1MN2NTC6aT61UklAiyw'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get PaperIDs and AuthorIDs for 13 queries about WNT Signalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though many of these queries return hundreds of thousands of papers, only the first 10,000 are available \n",
    "# due to a built in limit in AWS ElasticSearch. Could probably create a work around, but this is a sufficient \n",
    "# sample amount of paper and author IDs. From this, we can build out a network using paper IDs by author IDs \n",
    "# at a much faster rate due to having an API token. Cause this takes absolutely forever. \n",
    "\n",
    "data_WNT3SignallingGSK = get_data(\"WNT3+signalling\", offset, limit, NumResults)\n",
    "data_WNT5aSignalling = get_data(\"WNT5a+signalling\", offset, limit, NumResults)\n",
    "data_WNTSignallingVangl2 = get_data(\"WNT+signalling+Vangl2\", offset, limit, NumResults)\n",
    "data_WNTSignallingTCF = get_data(\"WNT+signalling+TCF\", offset, limit, NumResults)\n",
    "data_WNTSignallingLef = get_data(\"WNT+signalling+Lef\", offset, limit, NumResults)\n",
    "data_WNTSignallingPCP = get_data(\"WNT+signalling+PCP\", offset, limit, NumResults)\n",
    "data_WNTSignallingAxin = get_data(\"WNT+signalling+Axin\", offset, limit, NumResults)\n",
    "data_WNTSignallingCancer = get_data(\"WNT+signalling+Cancer\", offset, limit, NumResults)\n",
    "data_WNTSignallingStemCells = get_data(\"WNT+signalling+Stem+Cells\", offset, limit, NumResults)\n",
    "data_WNTSignallingFrizzled = get_data(\"WNT+signalling+Frizzled\", offset, limit, NumResults)\n",
    "data_WNTSignallingGSK = get_data(\"WNT+signalling+GSK\", offset, limit, NumResults)\n",
    "data_WNTSignallingBetaCatenin = get_data(\"WNT+signalling+Beta+Catenin\", offset, limit, NumResults)\n",
    "data_WNTSignalling = get_data(\"WNT+signalling\", offset, limit, NumResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_WNTSignallingVangl2),1)\n",
    "print(len(data_WNT3SignallingGSK),2)\n",
    "print(len(data_WNT5aSignalling),3)\n",
    "print(len(data_WNTSignallingTCF),4)\n",
    "print(len(data_WNTSignallingLef),5)\n",
    "print(len(data_WNTSignallingPCP),6)\n",
    "print(len(data_WNTSignallingAxin),7)\n",
    "print(len(data_WNTSignallingCancer),8)\n",
    "print(len(data_WNTSignallingStemCells),9)\n",
    "print(len(data_WNTSignallingFrizzled),10)\n",
    "print(len(data_WNTSignallingGSK),11)\n",
    "print(len(data_WNTSignallingBetaCatenin),12)\n",
    "print(len(data_WNTSignalling),13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create initial json dataset of paper and author IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data_WNTSignalling + data_WNTSignallingBetaCatenin + data_WNTSignallingPCP \n",
    "        + data_WNTSignallingAxin + data_WNTSignallingCancer + data_WNTSignallingStemCells \n",
    "        + data_WNTSignallingFrizzled + data_WNTSignallingGSK + data_WNTSignallingTCF \n",
    "        + data_WNTSignallingLef + data_WNTSignallingVangl2 + data_WNT5aSignalling + data_WNT3SignallingGSK)\n",
    "print(len(data))\n",
    "write_to_json(data, \"paper_and_authorIDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve unique Paper IDs from Initial Dataset, inspect total unique, and write to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperIDs = []\n",
    "with open(\"paper_and_authorIDs.json\", 'r', encoding='utf-8') as papers:\n",
    "    papers_data = json.load(papers)\n",
    "    for idx, paper in tqdm(enumerate(papers_data), total=len(papers_data)):        \n",
    "        try:\n",
    "            if paper[\"paperId\"]:\n",
    "                paperIDs.append(paper[\"paperId\"])\n",
    "            else: \n",
    "                continue\n",
    "        except:\n",
    "            print(sys.exc_info()[0])  \n",
    "            \n",
    "pID = set(paperIDs)\n",
    "paperIDs_final = list(pID)\n",
    "print(len(paperIDs_final))\n",
    "\n",
    "write_to_json(paperIDs_final, \"paperIDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve unique Author IDs from Initial Dataset, inspect total unique, and write to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorIDs = []\n",
    "with open(\"paper_and_authorIDs.json\", 'r', encoding='utf-8') as papers:\n",
    "    papers_data = json.load(papers)\n",
    "    for idx, paper in tqdm(enumerate(papers_data), total=len(papers_data)):        \n",
    "        try:\n",
    "            for author in paper[\"authors\"]:\n",
    "                try:\n",
    "                    if author[\"authorId\"]:\n",
    "                        paperIDs.append(author[\"authorId\"])\n",
    "                    else: \n",
    "                        continue\n",
    "                except:\n",
    "                    print(sys.exc_info()[0])  \n",
    "        except:   \n",
    "            print(sys.exc_info()[0])\n",
    "            \n",
    "aID = set(authorIDs)\n",
    "authorIDs_final = list(aID)\n",
    "print(len(authorIDs_final))\n",
    "\n",
    "write_to_json(authorIDs_final, \"authorIDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list variables of unique AuthorIDs and PaperIDs, from previous jsons, used for API requests (instead of re-running all previous cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"authorIDs.json\", 'r', encoding='utf-8') as author_json:\n",
    "    authorIDs = json.load(author_json)\n",
    "\n",
    "print(\"unique authorIDs:\", len(authorIDs))\n",
    "\n",
    "with open(\"paperIDs.json\", 'r', encoding='utf-8') as paper_json:\n",
    "    paperIDs = json.load(paper_json)\n",
    "\n",
    "print(\"unique paperIDs:\", len(paperIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all information for each PaperID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this took 12hrs to complete with API key\n",
    "\n",
    "sch = SemanticScholar(api_key=s2_api_key)\n",
    "papers_all_info = []\n",
    "\n",
    "for id in range(0, len(paperIDs)):\n",
    "    try:\n",
    "        if not (id / 100).is_integer(): \n",
    "            papers_all_info.append(sch.paper(paperIDs[id]))\n",
    "            id += 1\n",
    "        else: \n",
    "            papers_all_info.append(sch.paper(paperIDs[id]))\n",
    "            print(f'paper info retrieved: {id}')\n",
    "            id += 1\n",
    "    \n",
    "    except:\n",
    "        print(sys.exc_info()[0], \"writing to json, sleep 20 sec, continue\", id)\n",
    "        write_to_json(papers_all_info, \"papers\")\n",
    "        time.sleep(60) # to work around 403 errors\n",
    "\n",
    "write_to_json(papers_all_info, \"papers_corpus\")\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all author information by AuthorID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this took 36hrs to complete with API key\n",
    "\n",
    "sch = SemanticScholar(api_key=s2_api_key)\n",
    "authors_all_info = []\n",
    "\n",
    "for id in range(0, len(authorIDs)):\n",
    "    try:\n",
    "        if not (id / 100).is_integer(): \n",
    "            authors_all_info.append(sch.author(authorIDs[id]))\n",
    "            id += 1\n",
    "\n",
    "        else: \n",
    "            authors_all_info.append(sch.author(authorIDs[id]))\n",
    "            print(f'author info retrieved: {id}')\n",
    "            id += 1\n",
    "    \n",
    "    except:\n",
    "        print(sys.exc_info()[0], \"writing to json, sleep 30 sec, continue\", id)\n",
    "        write_to_json(authors_all_info, \"authors1\")\n",
    "        time.sleep(30) # to work around 403 errors\n",
    "\n",
    "write_to_json(authors_all_info, \"author_corpus\")\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect how many unique results for full paper and author information (should be same, or close, to authorIDs abnd paperIDs list above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"papers_corpus.json\", 'r', encoding='utf-8') as paper_json:\n",
    "    papers = json.load(paper_json)\n",
    "\n",
    "print(\"unique papers:\", len(papers))\n",
    "\n",
    "with open(\"authors_corpus.json\", 'r', encoding='utf-8') as authors_json:\n",
    "    authors = json.load(authors_json)\n",
    "\n",
    "print(\"unique authors:\", len(authors))\n",
    "\n",
    "#### the original api pull required batches to get the full author_corpus, below code is for knitting together batches\n",
    "\n",
    "# authors_list = []\n",
    "# authors = [\"authors1.json\", \"authors2.json\", \"authors3.json\"]\n",
    "# for i in authors:\n",
    "#     with open(i, 'r', encoding='utf-8') as author_json:\n",
    "#         auth = json.load(author_json)\n",
    "#         #print(len(auth))\n",
    "#     authors_list += auth\n",
    "# print(len(authors_list))\n",
    "\n",
    "# write_to_json(authors_list, \"author_corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving all unique paperIDs from author_corpus information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperIDs = []\n",
    "with open(\"authors_corpus\", 'r', encoding='utf-8') as papers:\n",
    "    papers_data = json.load(papers)\n",
    "    for idx, paper in tqdm(enumerate(papers_data), total=len(papers_data)):        \n",
    "        try:\n",
    "            for i in paper[\"papers\"]:\n",
    "                try:\n",
    "                    if i[\"paperId\"]:\n",
    "                        paperIDs.append(i[\"paperId\"])\n",
    "                    else: \n",
    "                        continue\n",
    "                except:\n",
    "                    print(sys.exc_info()[0])\n",
    "        except:\n",
    "            print(sys.exc_info()[0])\n",
    "                      \n",
    "pID = set(paperIDs)\n",
    "paperIDs_corpus = list(pID)\n",
    "print(\"unique paperIDs\", len(paperIDs_corpus)) ## 7,239,766 papers for 146,853 authors from 39,249 initial papers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking a random sample of paperIDs and authorIDs for testing; excluding authors of the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2022-2-1)\n",
    "\n",
    "with open(\"papers_corpus.json\", 'r', encoding='utf-8') as paper_json:\n",
    "    papers_corpus = json.load(paper_json)\n",
    "    papers_corpus_sample = random.sample(papers_corpus,1000)\n",
    "\n",
    "write_to_json(papers_corpus_sample, \"papers_corpus_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2022-2-1)\n",
    "\n",
    "with open(\"papers_corpus_sample.json\", 'r', encoding='utf-8') as paper_json:\n",
    "    papers_corpus_sample = json.load(paper_json)\n",
    "\n",
    "authorIDs_remove = []\n",
    "for paper in papers_corpus_sample:\n",
    "    for author in paper[\"authors\"]:\n",
    "        authorIDs_remove.append(author[\"authorId\"])\n",
    "\n",
    "aID = set(authorIDs_remove)\n",
    "authorIDs_remove = list(aID)\n",
    "\n",
    "with open(\"authorIDs.json\", 'r', encoding='utf-8') as authors_json:\n",
    "    authorIDs = json.load(authors_json)\n",
    "\n",
    "authorIDs_updated = [id for id in authorIDs if id not in authorIDs_remove]\n",
    "\n",
    "authorIDs_sample = random.sample(authorIDs_updated,5000)\n",
    "\n",
    "write_to_json(authorIDs_sample, \"authorIDs_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving and Appending PaperIDs to sample of AuthorIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"authorIDs_sample.json\", 'r', encoding='utf-8') as author_json:\n",
    "    authorIDs = json.load(author_json)\n",
    "\n",
    "with open(\"authors_corpus.json\", 'r', encoding='utf-8') as author_json2:\n",
    "    paperIDs_lookup = json.load(author_json2)\n",
    "\n",
    "badRecords = []\n",
    "papers = []\n",
    "authorCorpusLookup = {}\n",
    "\n",
    "for i in authorIDs:\n",
    "    for j in paperIDs_lookup:\n",
    "        try:\n",
    "            if j[\"authorId\"] == str(i): \n",
    "                for k in j[\"papers\"]:\n",
    "                    papers.append(k[\"paperId\"])                \n",
    "                authorCorpusLookup[i] = papers\n",
    "                papers = []\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            badRecords.append(j)\n",
    "\n",
    "write_to_json(authorCorpusLookup, \"authorCorpusLookup\")\n",
    "\n",
    "#### List comprehension to speed this up, just needs error handling and abstraction\n",
    "\n",
    "# def search(name, people):\n",
    "#     return [element for element in people if element['name'] == name]\n",
    "\n",
    "#### super quick nested list comprehension, but can't seem to get lengths... come back later\n",
    "#papers = [[i[\"papers\"][j][\"paperId\"] for i in paperIDs_lookup[1:2]] for j in range(0, len(i[\"papers\"]))]\n",
    "\n",
    "#print(paperIDs_lookup.keys())\n",
    "\n",
    "# papers = [i for i in paperIDs_lookup if i[\"authorId\"] == \"2064217435\"]\n",
    "# papers = [i[\"papers\"] for i in paperIDs_lookup]\n",
    "# paperIDs = [[j[l][\"paperId\"] for j in papers] for l in range(0, len(papers[0]))]\n",
    "\n",
    "#print([[j for j in i] for i in paperIDs_lookup])\n",
    "#print(len(paperIDs_lookup))\n",
    "\n",
    "#### THE KEY DOESN'T EXIST IN SOME RECORDS, NEED TO CLEAN JSON FILE ###\n",
    "#print([i[\"papers\"] for i in paperIDs_lookup[0:len(paperIDs_lookup)-1]])\n",
    "\n",
    "# print(type(papers))\n",
    "# print(len(papers[0]))\n",
    "# print(papers[0][\"papers\"])\n",
    "# print(len(paperIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the sample authorID corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"authorCorpusLookup.json\", 'r', encoding='utf-8') as paper_json:\n",
    "#     authorCorpusLookup = json.load(paper_json)\n",
    "\n",
    "sch = SemanticScholar(api_key=s2_api_key)\n",
    "count = 0\n",
    "AuthDict = {}\n",
    "PapersDict = {}\n",
    "papers = []\n",
    "failed = []\n",
    "authors_corpus_sample = []\n",
    "\n",
    "for authorid in authorCorpusLookup:\n",
    "    count += 1  \n",
    "    try:\n",
    "        for paperId in authorCorpusLookup[authorid]:\n",
    "            paperInfo = sch.paper(paperId)\n",
    "            #print(authorid, paperId)\n",
    "\n",
    "            if paperInfo[\"abstract\"]:\n",
    "                PapersDict[\"paperId\"] = paperId\n",
    "                PapersDict[\"abstract\"] = paperInfo[\"abstract\"]\n",
    "                PapersDict[\"topics\"] = paperInfo[\"topics\"]\n",
    "                PapersDict[\"s2FieldsOfStudy\"] = paperInfo[\"s2FieldsOfStudy\"]\n",
    "                PapersDict[\"fieldsOfStudy\"] = paperInfo[\"fieldsOfStudy\"]\n",
    "                papers.append(PapersDict)\n",
    "                PapersDict = {}\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        AuthDict[\"authorId\"] = authorid\n",
    "        AuthDict[\"papers\"] = papers\n",
    "        #print(AuthDict)\n",
    "        authors_corpus_sample.append(AuthDict)\n",
    "        AuthDict = {}\n",
    "        papers = []\n",
    "\n",
    "    except:\n",
    "        print(sys.exc_info()[0], \"writing to json, sleep 30 sec, continue\", count)\n",
    "        write_to_json(authors_corpus_sample, \"authors_corpus_sample\")\n",
    "        failed.append(authorid)\n",
    "        time.sleep(30) # to work around 403 errors    \n",
    "    \n",
    "write_to_json(authors_corpus_sample, \"authors_corpus_sample\")\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01dcfc14ced2cd3f3ca8b1eab5863829d3299fc9b429a7e61faa3aaa90451b9a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('NLP-startup': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
