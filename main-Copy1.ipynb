{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Isbu92y4wotj"
   },
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1636560301282,
     "user": {
      "displayName": "Abdullah Al-Hayali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgK_K6wHZSws-HNn245LDdwrHXRbD_gDyudUbqnIw=s64",
      "userId": "13392226524496428539"
     },
     "user_tz": 300
    },
    "id": "-nbvNWxGrOLI"
   },
   "outputs": [],
   "source": [
    "# update your credentials if needed\n",
    "!git config --global user.email \"abduallahw10@gmail.com\"\n",
    "!git config --global user.name \"Abdullah Al-Hayali\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4629,
     "status": "ok",
     "timestamp": 1636560307218,
     "user": {
      "displayName": "Abdullah Al-Hayali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgK_K6wHZSws-HNn245LDdwrHXRbD_gDyudUbqnIw=s64",
      "userId": "13392226524496428539"
     },
     "user_tz": 300
    },
    "id": "NV30BDR6wBQD",
    "outputId": "2fef5d4b-9d8b-4440-e2b4-77d235fc6e90"
   },
   "outputs": [],
   "source": [
    "# check that we're in the right repo, branch and that we are caught up\n",
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21zyxRtowk2l"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1636560311795,
     "user": {
      "displayName": "Abdullah Al-Hayali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgK_K6wHZSws-HNn245LDdwrHXRbD_gDyudUbqnIw=s64",
      "userId": "13392226524496428539"
     },
     "user_tz": 300
    },
    "id": "BnTeqTKbu05j"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "executionInfo": {
     "elapsed": 25262,
     "status": "ok",
     "timestamp": 1636560339580,
     "user": {
      "displayName": "Abdullah Al-Hayali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgK_K6wHZSws-HNn245LDdwrHXRbD_gDyudUbqnIw=s64",
      "userId": "13392226524496428539"
     },
     "user_tz": 300
    },
    "id": "vAqm8fSvyY4v",
    "outputId": "febbc0ab-7b1f-43b1-fe32-9f41f1826c99",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading metadata\n",
    "\n",
    "root_path = \"C:\\My files\\Courses\\CIS6050\\Data\"\n",
    "metadata_path = f'{root_path}/metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1636560474600,
     "user": {
      "displayName": "Abdullah Al-Hayali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgK_K6wHZSws-HNn245LDdwrHXRbD_gDyudUbqnIw=s64",
      "userId": "13392226524496428539"
     },
     "user_tz": 300
    },
    "id": "6L4NtJdayy9H",
    "outputId": "550174c3-2a21-4e85-b087-2abc7f07a56b"
   },
   "outputs": [],
   "source": [
    "# Entries for each column in the DF\n",
    "\n",
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKtW2pLcGotf"
   },
   "source": [
    "## Fetch JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 471417,
     "status": "ok",
     "timestamp": 1636558955978,
     "user": {
      "displayName": "Abdullah Al-Hayali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgK_K6wHZSws-HNn245LDdwrHXRbD_gDyudUbqnIw=s64",
      "userId": "13392226524496428539"
     },
     "user_tz": 300
    },
    "id": "zaiM3_F3HBnR",
    "outputId": "3b02e43a-4159-4519-8367-298668697170"
   },
   "outputs": [],
   "source": [
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "len(all_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1636477347598,
     "user": {
      "displayName": "Abdullah Al-Hayali",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgK_K6wHZSws-HNn245LDdwrHXRbD_gDyudUbqnIw=s64",
      "userId": "13392226524496428539"
     },
     "user_tz": 300
    },
    "id": "TO_LOxlZdoGo",
    "outputId": "e986f6d7-b01b-446c-ef4b-f988ee13b959"
   },
   "outputs": [],
   "source": [
    "all_json[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sRjNiuPo4ij",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# File reader class\n",
    "\n",
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.author_info = []\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            # Author\n",
    "            for entry in content['metadata']['authors']:\n",
    "                self.author_info.append(entry['first'])\n",
    "                self.author_info.append(entry['last'])\n",
    "#             print(self.author_info)\n",
    "            # Abstract\n",
    "            for entry in content['abstract']:\n",
    "                self.abstract.append(entry['text'])\n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                self.body_text.append(entry['text'])\n",
    "#             self.author_info = '\\n'.join(self.author_info)\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'AUTHOR: {self.author_info}; PAPER ID: {self.paper_id}; ABSTRACT: {self.abstract[:50]}; BODY TEXT: {self.body_text[:50]}'\n",
    "\n",
    "first_row = FileReader(all_json[2])\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter non-complying JSONS\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_json_clean = list()\n",
    "\n",
    "for idx, content in tqdm(enumerate(all_json)):    \n",
    "    try:\n",
    "        content = FileReader(content)\n",
    "    except Exception as e:\n",
    "        continue  # invalid paper format, skip\n",
    "    \n",
    "    if len(content.abstract) == 0:\n",
    "        continue\n",
    "    \n",
    "    all_json_clean.append(all_json[idx])\n",
    "    \n",
    "all_json = all_json_clean\n",
    "len(all_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'authors': [], \n",
    "         'title': [], 'journal': []}\n",
    "for idx, entry in tqdm(enumerate(all_json), total = len(all_json)):\n",
    "    \n",
    "    try:\n",
    "        content = FileReader(entry)\n",
    "    except Exception as e:\n",
    "        continue  # invalid paper format, skip\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    if len(content.body_text) == 0:\n",
    "        continue\n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "                \n",
    "    try:\n",
    "        # if more than one author\n",
    "        authors = meta_data['authors'].values[0].split(';')\n",
    "\n",
    "        dict_['authors'].append(\". \".join(authors))\n",
    "    except Exception as e:\n",
    "        # if only one author - or Null\n",
    "        dict_['authors'].append(meta_data['authors'].values[0])\n",
    "    \n",
    "    # add title\n",
    "    dict_['title'].append(meta_data['title'].values[0])\n",
    "    \n",
    "    # add the journal information\n",
    "    dict_['journal'].append(meta_data['journal'].values[0])\n",
    "    \n",
    "    # add doi\n",
    "    dict_['doi'].append(meta_data['doi'].values[0])\n",
    "    \n",
    "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'authors',\n",
    "                                        'title', 'journal'])\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = pd.read_csv(\"C:\\My files\\Courses\\CIS6050\\mod_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "\n",
    "The data isn't only English! here's the breakdown (https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/139146):\n",
    "\n",
    "('en', 28575),\n",
    "('fr', 323),\n",
    "('es', 281),\n",
    "('de', 54),\n",
    "('it', 19),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# hold label - language\n",
    "languages = []\n",
    "\n",
    "# go through each text\n",
    "for ii in tqdm(range(0,len(df_covid))):\n",
    "    # split by space into list, take the first x intex, join with space\n",
    "    text = df_covid.iloc[ii]['abstract'].split(\" \")\n",
    "    \n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    # ught... beginning of the document was not in a good format\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        # what!! :( let's see if we can find any text in abstract...\n",
    "        except Exception as e:\n",
    "            \n",
    "            try:\n",
    "                # let's try to label it through the abstract then\n",
    "                lang = detect(df_covid.iloc[ii]['abstract'])\n",
    "            except Exception as e:\n",
    "                lang = \"unknown\"\n",
    "                pass\n",
    "    \n",
    "    # get the language    \n",
    "    languages.append(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "languages_dict = {}\n",
    "for lang in set(languages):\n",
    "    languages_dict[lang] = languages.count(lang)\n",
    "    \n",
    "print(\"Total: {}\\n\".format(len(languages)))\n",
    "pprint(languages_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['language'] = languages\n",
    "df_covid = df_covid[df_covid['language'] == 'en'] \n",
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In case of reloading, START HERE and load clean_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_sci_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid=pd.read_csv(\"C:\\My files\\Courses\\CIS6050\\clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing topic modelling, the word embeddings are critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove noisy data\n",
    "stopwords = list(STOP_WORDS)\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can increase accuracy by adding custom stopwords but we'll skip them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text parsing to change everything to lowercase, remove punctuation, find and remove stopwords\n",
    "# en_core_sci_lg will be used to process biomedical text\n",
    "\"https://allenai.github.io/scispacy/\"\n",
    "#https://github.com/allenai/scispacy SciBERT could be a future suggestion\n",
    "\n",
    "\n",
    "parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"]) \n",
    "parser.max_length = 7000000\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df_covid[\"processed_text\"] = df_covid[\"abstract\"].progress_apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf will be used to convert the string into a measure of importance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def vectorize(text, maxx_features):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=maxx_features)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data\n",
    "\n",
    "text = df_covid['processed_text'].values\n",
    "#not sure how this works. Got it from the discussions\n",
    "max_words = 2**10 #max number of features/words of interest\n",
    "\n",
    "X = vectorize(text, max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Try without PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much we can reduce while we keep 95% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_reduced= pca.fit_transform(X.toarray())\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means clustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow method\n",
    "# https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# run kmeans with many different k\n",
    "distortions = []\n",
    "K = range(2, 21)\n",
    "for k in K:\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n",
    "    k_means.fit(X_reduced)\n",
    "    distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_reduced)\n",
    "df_covid['y_pred_k5'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 19\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_reduced)\n",
    "df_covid['y_pred_k19'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = df_covid.drop('y_pred',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(\"df_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN df_clean.csv here, you can ignore everything beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create vectorizers for the k clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = []\n",
    "    \n",
    "for ii in range(0, 20):\n",
    "    # Creating a vectorizer\n",
    "    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorize the data from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data for each vector\n",
    "\n",
    "vectorized_data = []\n",
    "\n",
    "for current_cluster, cvec in enumerate(vectorizers):\n",
    "    try:\n",
    "        vectorized_data.append(cvec.fit_transform(df_covid.loc[df_covid['y_pred_k19'] == current_cluster, 'processed_text']))\n",
    "    except Exception as e:\n",
    "        print(\"Not enough instances in cluster: \" + str(current_cluster))\n",
    "        vectorized_data.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics per cluster, this was RANDOMLY selected\n",
    "NUM_TOPICS_PER_CLUSTER = 20\n",
    "\n",
    "\n",
    "lda_models = []\n",
    "\n",
    "for ii in range(0, 20):\n",
    "    # Latent Dirichlet Allocation Model\n",
    "    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n",
    "    lda_models.append(lda)\n",
    "    \n",
    "lda_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_lda_data = []\n",
    "\n",
    "for current_cluster, lda in enumerate(lda_models):\n",
    "    print(\"Current Cluster: \" + str(current_cluster))\n",
    "    \n",
    "    if vectorized_data[current_cluster] != None:\n",
    "        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_vectorizer, lda in enumerate(lda_models):\n",
    "    print(\"Current Cluster: \" + str(lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic \n",
    "#From discussion forums\n",
    "\n",
    "def selected_topics(model, vectorizer, top_n=3):\n",
    "    current_words = []\n",
    "    keywords = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for word in words:\n",
    "            if word[0] not in current_words:\n",
    "                keywords.append(word)\n",
    "                current_words.append(word[0])\n",
    "                \n",
    "    keywords.sort(key = lambda x: x[1])  \n",
    "    keywords.reverse()\n",
    "    return_values = []\n",
    "    for ii in keywords:\n",
    "        return_values.append(ii[0])\n",
    "    return return_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for current_vectorizer, lda in enumerate(lda_models):\n",
    "    print(\"Current Cluster: \" + str(current_vectorizer))\n",
    "\n",
    "    if vectorized_data[current_vectorizer] != None:\n",
    "        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(all_keywords,\"\\n\", len(all_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA for every single paper\n",
    "\n",
    "Adpoted from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid=pd.read_csv(\"C:\\My files\\Courses\\CIS6050\\df_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = df_covid['paper_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_s = df_covid['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_text_s.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates unique id for each word in the document. Its mapping of word_id and word_frequency. Example: (4,1) above indicates, word_id 4 occurs once in the document and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View\n",
    "print(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=NUM_TOPICS_PER_CLUSTER)\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDigits(lst):\n",
    "    return [[el] for el in lst]\n",
    "\n",
    "#Change a list of words to list of lists\n",
    "ll_words = extractDigits(data_words[0])\n",
    "\n",
    "# Create Dictionary\n",
    "id2word_t = corpora.Dictionary(ll_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts_t = ll_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_t = [id2word.doc2bow(text) for text in texts_t]    \n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus_t,\n",
    "                                       id2word=id2word_t,\n",
    "                                       num_topics=NUM_TOPICS_PER_CLUSTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDigits(lst):\n",
    "    return [[el] for el in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "NUM_TOPICS_PER_CLUSTER = 10\n",
    "abs_lda = []\n",
    "\n",
    "for i in tqdm(range(0,5)):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    #Change a list of words to list of lists\n",
    "    ll_words = extractDigits(data_words[i])\n",
    "#     print(ll_words)\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word_t = corpora.Dictionary(ll_words)\n",
    "#     print(id2word_t)\n",
    "    \n",
    "    # Create Corpus\n",
    "    texts_t = ll_words\n",
    "    print(texts_t)\n",
    "    \n",
    "    # Term Document Frequency\n",
    "    corpus_t = [id2word_t.doc2bow(text) for text in texts_t]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus_t,\n",
    "                                           id2word=id2word_t,\n",
    "                                           num_topics=NUM_TOPICS_PER_CLUSTER)\n",
    "        \n",
    "    abs_lda.append(lda_model[corpus_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[abs_lda(1)]\n",
    "\n",
    "abs_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next cells export the filtered all_json list into a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_jj = all_json[:]\n",
    "len(new_jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the longest prefix of all list elements.\n",
    "def commonprefix(m):\n",
    "    \"Given a list of pathnames, returns the longest common leading component\"\n",
    "    if not m: return ''\n",
    "    s1 = min(m)\n",
    "    s2 = max(m)\n",
    "    for i, c in enumerate(s1):\n",
    "        if c != s2[i]:\n",
    "            return s1[:i]\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonprefix(new_jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test22 = pd.DataFrame(new_jj)\n",
    "df_test22.to_csv('Paths_JSON_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap code\n",
    "\n",
    "print(\"empty count\", meta_df['abstract'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_df['abstract'].notnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git branch \"a_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout \"a_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git push origin a_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git commit -m\"More clean df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "01dcfc14ced2cd3f3ca8b1eab5863829d3299fc9b429a7e61faa3aaa90451b9a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
