{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Papers Corpus for Embedding and Semantics Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  100 unique paper abstracts\n",
    "with open(\"Test-Data/papers_corpus_test.json\", 'r', encoding='utf-8') as Papers:\n",
    "    Papers = json.load(Papers)\n",
    "    dfPapers = pd.DataFrame.from_dict(Papers, orient='columns')\n",
    "\n",
    "# create paper subset for embedding and semantic matching\n",
    "dfPapersCorpus = dfPapers[[\"paperId\", \"abstract\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect nulls and drop\n",
    "dfPapersNulls = dfPapersCorpus[dfPapersCorpus.isnull().any(axis=1)]\n",
    "dfPapersCorpus = dfPapersCorpus.drop(dfPapersNulls.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfPapersCorpus.loc[dfPapersCorpus['paperId'] == '3ffd20f1b61313d5b17d6b5db1a144d8e664e968'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Authors Corpus for Embedding and Semantics Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  90 unique authors with ~4100 papers. About 45 papers per author \n",
    "with open(\"Test-Data/authors_corpus_test.json\", 'r', encoding='utf-8') as Authors:\n",
    "    Authors = json.load(Authors)\n",
    "    dfAuthors = pd.json_normalize(Authors, record_path=['papers'], meta='authorId')\n",
    "\n",
    "# create paper subset for embedding and semantic matching\n",
    "dfAuthorsCorpus = dfAuthors[[\"authorId\", \"abstract\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all abstracts into single cell for each author\n",
    "dfAuthorsCorpus = dfAuthorsCorpus.groupby(['authorId'], as_index=False).agg({'abstract': ' '.join})\n",
    "\n",
    "# inspect the contents of one of the cells\n",
    "print(dfAuthorsCorpus.loc[dfAuthorsCorpus['authorId'] == '40544263'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT input (replace all this and below with pytorch)\n",
    "author_abstract = dfAuthorsCorpus.abstract.values\n",
    "author_labels = dfAuthorsCorpus.authorId.values\n",
    "paper_abstract = dfPapersCorpus.abstract.values\n",
    "\n",
    "print(len(author_abstract))\n",
    "print(len(author_labels))\n",
    "print(len(paper_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "top_n_words = 10\n",
    "np.random.seed(2021-12-30)\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "top_n_words = 10\n",
    "np.random.seed(2021-12-30)\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "auth2paper_final = []\n",
    "#for i in tqdm(range(0, 10)): \n",
    "for i in tqdm(range(0, len(author_abstract))): \n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([author_abstract[i]])\n",
    "    candidates = count.get_feature_names()\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "    \n",
    "    auth2paper_cosim = []\n",
    "    \n",
    "    #for k in range(0, 20): \n",
    "    for k in range(0, len(paper_abstract)):   \n",
    "        doc_embedding = model.encode([paper_abstract[k]])\n",
    "        cosine_sim = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "        #top_n_keywords = [candidates[index] for index in cosine_sim.argsort()[0][-top_n_words:]]\n",
    "        top_n_candidate_embeddings = np.reshape(np.mean(np.array([candidate_embeddings[index] for index in cosine_sim.argsort()[0][-top_n_words:]]), axis=0), (1, -1))\n",
    "        #top_n_candidate_embeddings = np.reshape(arr_list, (1, -1))\n",
    "        cosine_sim_top_n = cosine_similarity(doc_embedding, top_n_candidate_embeddings)\n",
    "        unravelled = float(np.ravel(cosine_sim_top_n))\n",
    "        auth2paper_cosim.append(unravelled)\n",
    "\n",
    "    auth2paper_final.append(auth2paper_cosim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_index = []\n",
    "paper_top_scores = []\n",
    "\n",
    "for i in tqdm(range(0,len(auth2paper_final))):\n",
    "    sample_list1 = [] \n",
    "    sample_list2 = [] \n",
    "    for index, value in sorted(enumerate(auth2paper_final[i]), reverse=True, key=lambda x: x[1])[:10]:\n",
    "        sample_list1.append(index)\n",
    "        sample_list2.append(value)\n",
    "    paper_index.append(sample_list1)\n",
    "    paper_top_scores.append(sample_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_au_id <- clean author IDs\n",
    "final_df = pd.DataFrame({'Author ID': dfAuthorsCorpus['authorId'],\n",
    "                         'Top 10 Cosine Similarity Scores': paper_top_scores,\n",
    "                         'Paper Indices':paper_index\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"BERT-SemanticMatching_Results-Test.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f0250bab1e3efeb4191379c3ad9bbc1241ec68ec80bf975fb8e1e10c77b2ab6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('NLPthesis-01')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
