{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_sci_lg\n",
    "import os\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import ldamodel\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON file loader to DF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the following cells only if the csv file isn't available and JSON is on hand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AuthAbs_full.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the first half of the JSONS\n",
    "pd_authabs1 = pd.DataFrame.from_dict(data[0], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_authabs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the second part of the JSONS\n",
    "pd_authabs2 = pd.DataFrame.from_dict(data[1], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_authabs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the two dataframes\n",
    "\n",
    "pd_authabs = pd.concat([pd_authabs1,pd_authabs2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_authabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the the row that's full of Nulls/nans\n",
    "new_df = pd_authabs.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign column names\n",
    "new_df.columns = ['Abstract','1','2','3','4','5','6','7','8','9','10','11','12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the index column to an Author ID column. This will be later changed to an independent column for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.index.name = 'Author ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Author ID'] = new_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset indices to 0 (replacing the AuthorID that were previously preceived as indices)\n",
    "new_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column that's a concatenation of all text columns\n",
    "new_df['Abstract_new'] = new_df['Abstract'].astype(str) + new_df['1'].astype(str) + new_df['2'].astype(str) + new_df['3'].astype(str) + new_df['4'].astype(str) + new_df['5'].astype(str) + new_df['6'].astype(str) + new_df['7'].astype(str) + new_df['8'].astype(str) + new_df['9'].astype(str) + new_df['10'].astype(str) + new_df['11'].astype(str) + new_df['12'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(['Abstract','1','2','3','4','5','6','7','8','9','10','11','12'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"AuthorAbs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the DF cleaning is done, LDA work is next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in author abstracts\n",
    "new_df = pd.read_csv('AuthorAbs.csv')\n",
    "\n",
    "#create one columned df with abstracts only\n",
    "df_text = new_df['Abstract_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load JSON files for the abstracts\n",
    "\n",
    "paper_abstracts = []\n",
    "with open(\"papers2.json\", 'r', encoding='utf-8') as papers:\n",
    "    papers = json.load(papers)\n",
    "    for j in papers:\n",
    "        if j:\n",
    "            try:        \n",
    "                paper_abstracts.append(j[\"abstract\"])\n",
    "            except:   \n",
    "                #print the error message from sys\n",
    "                print(\"error:\", sys.exc_info()[0])\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an LDA model for all the authors' papers\n",
    "\n",
    "Logic adopted from GENSIM's official tutorial (https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionary, word splitting adapted from (https://www.geeksforgeeks.org/python-program-split-join-string/)\n",
    "dictionary_a = corpora.Dictionary([\" \".join(df_text).split()]) \n",
    "print(f'{len(dictionary_a)} different terms in the corpus')\n",
    "#creating the bag of words object\n",
    "bow_corpus_a = [dictionary_a.doc2bow(text.split()) for text in df_text]\n",
    "\n",
    "#train LDA models\n",
    "lda_model_bow_a = models.LdaModel(corpus=bow_corpus_a, id2word=dictionary_a, num_topics=10,\n",
    "                                random_state=47)\n",
    "\n",
    "lda_model_bow_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lda_a = []\n",
    "\n",
    "for i in tqdm(range(0,len(df_text))):    \n",
    "    #attain dictionary for the abstract\n",
    "    abs_vec = dictionary_a.doc2bow(df_text[i].split())    \n",
    "    #extract topics from the LDA model\n",
    "    abs_lda_vec = lda_model_bow_a[abs_vec]\n",
    "    print (f'document {i} feature vector: ', abs_lda_vec)    \n",
    "#     pprint(lda_model_bow_a.print_topics(10, num_words=5)) #prints the topics with their respective top-word probability\n",
    "     \n",
    "    print('\\n')\n",
    "    abs_lda_a.append(abs_lda_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abs_lda_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#feature vectors of paper abstracts\n",
    "abstract_lda = []\n",
    "\n",
    "#load in the first 1000 abstracts\n",
    "for i in tqdm(range(0,len(paper_abstracts[:1000]))):\n",
    "    ab_ = dictionary_a.doc2bow(paper_abstracts[i].split())\n",
    "    abs_lda = lda_model_bow_a[ab_]\n",
    "    print ('document topics: ', abs_lda)\n",
    "    abstract_lda.append(abs_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing every author to all the papers, getting similarity scores and aggregating them onto a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# abs_lda_a: authors' abstracts feature vectors\n",
    "# abstract_lda: abstract feature vectors\n",
    "##################\n",
    "\n",
    "cos_scores = []\n",
    "\n",
    "for i in tqdm(range(0,len(abs_lda_a))):\n",
    "    sample_list = []\n",
    "    \n",
    "    for j in range(0,len(abstract_lda)):\n",
    "        sample_list.append(gensim.matutils.cossim(abs_lda_a[i],abstract_lda[j]))\n",
    "    \n",
    "    cos_scores.append(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list to extract the top 10 abstracts for each author\n",
    "indices = []\n",
    "top_scores = []\n",
    "\n",
    "for i in tqdm(range(0,len(cos_scores))):\n",
    "    sample_list1 = []\n",
    "    sample_list2 = []\n",
    "    for index, value in sorted(enumerate(cos_scores[i]), reverse=True, key=lambda x: x[1])[:10]:\n",
    "        sample_list1.append(index)\n",
    "        sample_list2.append(value)\n",
    "    indices.append(sample_list1)\n",
    "    top_scores.append(sample_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a DF that contains AuthorIDs, their respective top 10 cosine indices and the paper indicies, and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Author ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_au_id <- clean author IDs\n",
    "\n",
    "final_df = pd.DataFrame({'Author ID':new_df['Author ID'],\n",
    "                         'Top 10 Cosine Similarity Scores': top_scores,\n",
    "                         'Paper Indices':indices\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the first 100 authors\n",
    "final_df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[:100].to_csv('final_cossim.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
