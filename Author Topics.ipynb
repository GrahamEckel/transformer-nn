{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_sci_lg\n",
    "import os\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import ldamodel\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AuthAbs_full.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the first half of the JSONS\n",
    "pd_authabs1 = pd.DataFrame.from_dict(data[0], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_authabs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the second part of the JSONS\n",
    "pd_authabs2 = pd.DataFrame.from_dict(data[1], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_authabs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the two dataframes\n",
    "\n",
    "pd_authabs = pd.concat([pd_authabs1,pd_authabs2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_authabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the the row that's full of Nulls/nans\n",
    "new_df = pd_authabs.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign column names\n",
    "new_df.columns = ['Abstract','1','2','3','4','5','6','7','8','9','10','11','12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the index column to an Author ID column. This will be later changed to an independent column for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.index.name = 'Author ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Author ID'] = new_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset indices to 0 (replacing the AuthorID that were previously preceived as indices)\n",
    "new_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column that's a concatenation of all text columns\n",
    "new_df['Abstract_new'] = new_df['Abstract'].astype(str) + new_df['1'].astype(str) + new_df['2'].astype(str) + new_df['3'].astype(str) + new_df['4'].astype(str) + new_df['5'].astype(str) + new_df['6'].astype(str) + new_df['7'].astype(str) + new_df['8'].astype(str) + new_df['9'].astype(str) + new_df['10'].astype(str) + new_df['11'].astype(str) + new_df['12'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(['Abstract','1','2','3','4','5','6','7','8','9','10','11','12'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"AuthorAbs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the DF cleaning is done, LDA work is next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one columned df with abstracts only\n",
    "df_text = new_df['Abstract_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert sentences to individual words\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w = df_text.values.tolist()\n",
    "data_words = list(sent_to_words(data_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDigits(lst):\n",
    "    return [[el] for el in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "NUM_TOPICS_PER_CLUSTER = 10\n",
    "\n",
    "abs_lda = []\n",
    "\n",
    "for i in tqdm(range(0,len(data_words))):\n",
    "    \n",
    "    if not data_words[i]:\n",
    "        continue   \n",
    "    #Change a list of words to list of lists\n",
    "    ll_words = extractDigits(data_words[i])\n",
    "    # Create Dictionary\n",
    "    id2word_t = corpora.Dictionary(ll_words)    \n",
    "    # Create Corpus\n",
    "    texts_t = ll_words   \n",
    "    # Term Document Frequency\n",
    "    corpus_t = [id2word_t.doc2bow(text) for text in texts_t]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus_t,\n",
    "                                           id2word=id2word_t,\n",
    "                                           num_topics=NUM_TOPICS_PER_CLUSTER)\n",
    "        \n",
    "    abs_lda.append(lda_model[corpus_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify any empty list entries (placeholders for the abstracts)\n",
    "\n",
    "empty = []\n",
    "\n",
    "for i in tqdm(range(0,len(data_words))):\n",
    "    if not data_words[i]:\n",
    "        print(f\"datapoint {i} is empty\")\n",
    "        empty.append(i)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up the author id df from the empty abstracts\n",
    "\n",
    "df_au_id = new_df['Author ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_au_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_au_id = df_au_id.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean authorid df\n",
    "df_au_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lda[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abs_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abs = abs_lda[0]\n",
    "# len(test_abs)\n",
    "for topic in test_abs:\n",
    "    print(topic)\n",
    "\n",
    "# print(topic for topic in test_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LDA model\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"abs_lda.txt\",\"wb\") as fp:\n",
    "    pickle.dump(abs_lda,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load JSON files\n",
    "\n",
    "paper_abstracts = []\n",
    "with open(\"papers.json\", 'r', encoding='utf-8') as papers:\n",
    "    papers = json.load(papers)\n",
    "    for j in papers:\n",
    "        if j:\n",
    "            try:        \n",
    "                paper_abstracts.append(j[\"abstract\"])\n",
    "            except:   \n",
    "                #print the error message from sys\n",
    "                print(\"error:\", sys.exc_info()[0])\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(paper_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe from the list\n",
    "\n",
    "paper_abs_df = pd.DataFrame(paper_abstracts, columns=['Abstracts'])\n",
    "paper_abs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_abs_df['Abstracts'][0] #test access to abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the df sentences onto words\n",
    "sentences_abs = paper_abs_df.values.tolist()\n",
    "sentences_words = list(sent_to_words(sentences_abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inspect the words\n",
    "sentences_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS_PER_CLUSTER = 10\n",
    "\n",
    "papers_lda = []\n",
    "test_lda = []\n",
    "topics_lda = []\n",
    "\n",
    "for i in tqdm(range(0,len(sentences_words))):\n",
    "    \n",
    "    if not sentences_words[i]:\n",
    "        continue   \n",
    "    #Change a list of words to list of lists\n",
    "    ll_words = extractDigits(sentences_words[i])\n",
    "    # Create Dictionary\n",
    "    id2word_t = corpora.Dictionary(ll_words)    \n",
    "    # Create Corpus\n",
    "    texts_t = ll_words   \n",
    "    # Converting list of documents (corpus) into Document Term Matrix using \n",
    "    #id2word_t prepared above.\n",
    "    corpus_t = [id2word_t.doc2bow(text) for text in texts_t]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = models.LdaModel(corpus=corpus_t,\n",
    "                                       id2word=id2word_t,\n",
    "                                       num_topics=NUM_TOPICS_PER_CLUSTER,\n",
    "                                       random_state=47        \n",
    "                                      )\n",
    "        \n",
    "    papers_lda.append(lda_model[corpus_t])\n",
    "    topics = lda_model.get_topics()\n",
    "    topics_1 = lda_model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        test_lda.append(topic)\n",
    "    for topics in topics_1:\n",
    "        topics_lda.append(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(papers_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(papers_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints out the keywords for each topic for each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives you a full (sparse) array where each row is a topic, and each column a vocabulary word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/61596101/calculating-cosine-similarity-from-a-gensim-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(test_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttttt = test_lda[0:10]\n",
    "ttttt_1 = test_lda[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(ttttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt = cosine_similarity(ttttt_1, ttttt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_twt = pd.DataFrame(twt, columns=['Word1', 'Word2', 'Word3', 'Word4', 'Word5', 'Word6', 'Word7', 'Word8', 'Word9', 'Word10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_twt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_t = gensim.matutils.cossim(papers_lda[0][0],papers_lda[1][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS WHERE THE REAL WORK IS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an LDA model for all the authors' papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionary\n",
    "dictionary_a = corpora.Dictionary([\" \".join(df_text).split()]) \n",
    "print(f'{len(dictionary_a)} different terms in the corpus')\n",
    "#creating the bag of words object\n",
    "bow_corpus_a = [dictionary_a.doc2bow(text.split()) for text in df_text]\n",
    "\n",
    "#train LDA models\n",
    "lda_model_bow_a = models.LdaModel(corpus=bow_corpus_a, id2word=dictionary_a, num_topics=10,\n",
    "                                random_state=47)\n",
    "\n",
    "lda_model_bow_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lda_a = []\n",
    "\n",
    "for i in tqdm(range(0,len(df_text))):    \n",
    "    #attain dictionary for the abstract\n",
    "    abs_vec = dictionary_a.doc2bow(df_text[i].split())    \n",
    "    #extract topics from the LDA model\n",
    "    abs_lda_vec = lda_model_bow_a[abs_vec]\n",
    "    print (f'document {i} feature vector: ', abs_lda_vec)    \n",
    "#     pprint(lda_model_bow_a.print_topics(10, num_words=5)) #prints the topics with their respective top-word probability\n",
    "     \n",
    "    print('\\n')\n",
    "    abs_lda_a.append(abs_lda_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abs_lda_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#feature vectors of paper abstracts\n",
    "abstract_lda = []\n",
    "\n",
    "for i in tqdm(range(0,len(paper_abstracts))):\n",
    "    ab_ = dictionary_a.doc2bow(paper_abstracts[i].split())\n",
    "    abs_lda = lda_model_bow_a[ab_]\n",
    "    print ('document topics: ', abs_lda)\n",
    "    abstract_lda.append(abs_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing every author to all the papers, getting similarity scores and aggregating them onto a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# abs_lda_a: authors' abstracts feature vectors\n",
    "# abstract_lda: abstract feature vectors\n",
    "##################\n",
    "\n",
    "cos_scores = []\n",
    "\n",
    "for i in tqdm(range(0,len(abs_lda_a))):\n",
    "    sample_list = []\n",
    "    \n",
    "    for j in range(0,len(abstract_lda)):\n",
    "        sample_list.append(gensim.matutils.cossim(abs_lda_a[i],abstract_lda[j]))\n",
    "    \n",
    "    cos_scores.append(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list to extract the top 10 abstracts for each author\n",
    "indices = []\n",
    "top_scores = []\n",
    "\n",
    "for i in tqdm(range(0,len(cos_scores))):\n",
    "    sample_list1 = []\n",
    "    sample_list2 = []\n",
    "    for index, value in sorted(enumerate(cos_scores[i]), reverse=True, key=lambda x: x[1])[:10]:\n",
    "        sample_list1.append(index)\n",
    "        sample_list2.append(value)\n",
    "    indices.append(sample_list1)\n",
    "    top_scores.append(sample_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a DF that contains AuthorIDs, their respective top 10 cosine indices and the paper indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Author ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_au_id <- clean author IDs\n",
    "\n",
    "final_df = pd.DataFrame({'Author ID':new_df['Author ID'],\n",
    "                         'Top 10 Cosine Similarity Scores': top_scores,\n",
    "                         'Paper Indices':indices\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_cossim.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an LDA model for all the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionary\n",
    "dictionary = corpora.Dictionary([\" \".join(paper_abstracts).split()]) \n",
    "print(f'{len(dictionary)} different terms in the corpus')\n",
    "#creating the bag of words object\n",
    "bow_corpus = [dictionary.doc2bow(text.split()) for text in paper_abstracts]\n",
    "\n",
    "#train LDA models\n",
    "lda_model_bow = models.LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=10,\n",
    "                                passes=1, random_state=47)\n",
    "\n",
    "lda_model_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abs_lda_t = []\n",
    "\n",
    "for i in tqdm(range(0,len(paper_abstracts))):    \n",
    "    #attain dictionary for the abstract\n",
    "    abs_vec = dictionary.doc2bow(paper_abstracts[i].split())    \n",
    "    #extract topics from the LDA model\n",
    "    abs_lda_vec = lda_model_bow[abs_vec]\n",
    "    print (f'document {i} feature vector: ', abs_lda_vec)    \n",
    "    pprint(lda_model.print_topics(10, num_words=5)) #prints the topics with their respective top-word probability\n",
    "     \n",
    "    print('\\n')\n",
    "    abs_lda_t.append(abs_lda_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(paper_abstracts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
